## 학습(learning) 의 정의
DL = ML + nn
ML = 통계 + DL
## 확률의 정의
선험적 통계 = 사전, 수학적 확률, 식 -> 연역법
경험적 통계 = 사후,  통계적 확률, 식 * "큰수의 법칙" -> 귀납법
기대값 = 계수 * 변수 + 상수
## 변수는 feature 와 target 으로 나뉜다.
## 상수는 계수와 편향로 나뉜다.
## 따라서 다음과 같은 식의 구조를 같는다.
target = 계수 * feature-value + 편향
특성변수 = 독립변수 = 외생변수 = x변수
목적변수 = 종속변수 = 내생변수 = y변수
feature 의 변환은 표준화(Z-score 정규화)와 정규화가 있다.
아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.

- 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
- 보통 식 앞에 곱해지는 상수를 말한다.
- 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.

## 학습은 통계학에서  추정문제 해결과정(=추론)이다.
learning 은 target 을 구하는 modeling 이다.

## 데이터타입
카테고리(categorical) = 이산형 = norminal + ordinal = 정수형
숫자형(numeric) = 연속형 = ratio + interval = 실수형

# 데이터 분석에는 크게 두 가지의 접근방법
1) 확증적 데이터 분석(CDA: Confirmatory Data Analysis) = 추론통계 = 가설 -> .. -> 특정 사례 예측 = 연역
2) 탐색적 데이터 분석(EDA: Exploratory Data Analysis) = 기술통계 = 데이터 -> .. -> 모델 = 귀납

## ML 을 위한 통계개념
표본
우도함수
대수의 법칙
베이지안
분포
랜덤

## 지도학습은 샘플을 사용한다.
## 비지도학습은 샘플을 사용하지 않는다.

지도학습 분류 classification / 회귀 regress 로 나뉜다.
model 은 var 를 잡아내서, class 를 시도한다.

## (확률) 분포는 함수다
리턴값에 따라 정수는 PMF, 실수는 PDF 를 사용한다.
인공지능에서는 Dense 레이어를 사용하므로, 리턴값은 실수로 정의한다.
확률분포는
이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포

# 연역과 귀납
연역은 가정된 전제이다.
귀납은 개인적 경험이다.

# 편향과 편차
https://opentutorials.org/module/3653/22071
정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산' 입니다.

예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.

회귀 문제이든, 분류 문제이든
첫 번째 그림과 같은 상황을 Underfitting = High Bias
세 번째 그림과 같은 상황을 Overfitting이라고 합니다. = High Variance

## 추정에 있어 통계학의 손실함수에는 평균제곱오차 또는 음의 로그 우도함수가 있으며
   머신러닝에서도 동일한 손실함수를 사용한다.
## 우도함수: 우도 함수(가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)
는 실현된 데이터(혹은 관찰된 데이터 observed data)로 부터
특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.
## 손실함수 혹은 비용함수(cost function)는 같은 용어로 통계학, 경제학 등에서 널리 쓰이는 함수로
    머신러닝에서도 손실함수는 예측값과 실제값에 대한 오차를 줄이는 데 사용된다.
## MSE vs. CCEE
회귀ML 의 손실함수는 MSE 이다
분류ML 의 손실함수는 CCEE 이다. 활성화함수로 Softmax 를 사용한다.